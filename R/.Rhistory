allLabeled$consensusCount,
allLabeled$probabilityLabel,
allLabeled$consensusLabel,
allLabeled$python_sentiment,
allLabeled$ensemble_pos)
x <- cbind.data.frame(allLabeled$maxEntPrediction,
allLabeled$maxEntProbability,
allLabeled$svmPrediction,
allLabeled$svmProbability,
allLabeled$glmnetPrediction,
allLabeled$glmnetProbability,
allLabeled$consensusCount,
allLabeled$probabilityLabel,
allLabeled$consensusLabel,
allLabeled$python_sentiment)
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
x <- cbind.data.frame(allLabeled$maxEntPrediction,
allLabeled$maxEntProbability,
allLabeled$svmPrediction,
allLabeled$svmProbability,
allLabeled$glmnetPrediction,
allLabeled$glmnetProbability,
allLabeled$consensusCount,
allLabeled$probabilityLabel,
allLabeled$consensusLabel,
#allLabeled$python_sentiment,
allLabeled$ensemble_pos,
allLabeled$ensemble_neg,
allLabeled$lexicon_pos,
allLabeled$lexicon_neu,
allLabeled$ensemble_neg,
allLabeled$lexicon_compound)
colnames(x) <- c("maxEntPrediction", "maxEntProbability", "svmPrediction", "glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
allLabeled <- read.csv("labeledWithPredictionsQuoteEdit.csv", sep=",")
# Basically turn this into an 'is neutral' flag
first <-  gsub("Negative", "NotNeutral", allLabeled$sentiment)
second <- gsub("Positive", "NotNeutral", first)
#third <- gsub("Neutral", "1", second)
#y <- as.numeric(as.factor(third))
y <- as.factor(second)
print(paste("# rows read in = ", dim(allLabeled)[1]))
x <- cbind.data.frame(allLabeled$maxEntPrediction,
allLabeled$maxEntProbability,
allLabeled$svmPrediction,
allLabeled$svmProbability,
allLabeled$glmnetPrediction,
allLabeled$glmnetProbability,
allLabeled$consensusCount,
allLabeled$probabilityLabel,
allLabeled$consensusLabel,
#allLabeled$python_sentiment,
allLabeled$ensemble_pos,
allLabeled$ensemble_neg,
allLabeled$lexicon_pos,
allLabeled$lexicon_neu,
allLabeled$ensemble_neg,
allLabeled$lexicon_compound)
colnames(x) <- c("maxEntPrediction", "maxEntProbability", "svmPrediction", "glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
# sentiment,predictedLabel,actualLabel,maxEntPrediction,maxEntProbability,svmPrediction,svmProbability,glmnetPrediction,glmnetProbability,consensusCount,probabilityLabel,consensusLabel,python_sentiment,ensemble_pos,ensemble_neg,lexicon pos,lexicon_neu,ensemble_neg,lexicon_compound
# TODO: we have a bad value around 8625/8626
#x <- x[1:8600,]
#y <- y[1:8600]
#second <- second[1:8600]
model <- svm(x, y)
pred <- predict(model, x)
print(paste("Predictions made!"))
predAndY <- cbind.data.frame(pred, y, second)
print( table(pred, y) )
sample <- read.csv("Sample_CombinedResultsWithAllStats.csv")
sampy <- sample$X.correct..human.label
sampx <- cbind.data.frame(allLabeled$maxEntPrediction,
allLabeled$maxEntProbability,
allLabeled$svmPrediction,
allLabeled$svmProbability,
allLabeled$glmnetPrediction,
allLabeled$glmnetProbability,
allLabeled$consensusCount,
allLabeled$probabilityLabel,
allLabeled$consensusLabel,
#allLabeled$python_sentiment,
allLabeled$ensemble_pos,
allLabeled$ensemble_neg,
allLabeled$lexicon_pos,
allLabeled$lexicon_neu,
allLabeled$ensemble_neg,
allLabeled$lexicon_compound)
colnames(sampx) <- c("maxEntPrediction", "maxEntProbability", "svmPrediction", "glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
pred <- predict(model, x)
pred <- predict(model, sampx)
sampy
length(sampy)
sampy <- sample$X.correct..human.label[1:100]
sampy
sample <- read.csv("Sample_CombinedResultsWithAllStats.csv")
sampy <- sample$X.correct..human.label[1:100]
sample <- sample[1:100,]
sampx <- cbind.data.frame(sample$maxEntPrediction,
sample$maxEntProbability,
sample$svmPrediction,
sample$svmProbability,
sample$glmnetPrediction,
sample$glmnetProbability,
sample$consensusCount,
sample$probabilityLabel,
sample$consensusLabel,
#allLabeled$python_sentiment,
sample$ensemble_pos,
sample$ensemble_neg,
sample$lexicon_pos,
sample$lexicon_neu,
sample$ensemble_neg,
sample$lexicon_compound)
colnames(sampx) <- c("maxEntPrediction", "maxEntProbability", "svmPrediction", "glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
pred <- predict(model, sampx)
sampy[1]
firsty <- first <-  gsub("1", "NotNeutral", sampy)
sampy[1]
as.character(sampy)
firsty <- first <-  gsub("1", "NotNeutral", as.character(sampy))
firsty[1]
firsty <- first <-  gsub("1", "NotNeutral", sampy)
firsty[1]
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
firsty <- gsub("1", "NotNeutral", sampy)
firsty <- gsub("3", "NotNeutral", firsty)
firsty
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
firsty <- gsub("1", "NotNeutral", sampy)
firsty <- gsub("3", "NotNeutral", firsty)
firsty <- gsub("2", "Neutral", firsty)
firsty
pred <- predict(model, sampx)
firsty <- gsub("1", "NotNeutral", sampy)
firsty <- gsub("3", "NotNeutral", firsty)
firsty <- gsub("2", "Neutral", firsty)
print( table(pred, sampy) )
print( table(pred, firsty) )
sampy <- sample$X.correct..human.label[1:100]
sample <- sample[1:100,]
sampy <- sample$X.correct..human.label
firsty <- gsub("1", "NotNeutral", sampy)
firsty <- gsub("3", "NotNeutral", firsty)
firsty <- gsub("2", "Neutral", firsty)
sampy <- as.factor(firsty)
print( table(pred, sampy) )
sample <- read.csv("Sample_CombinedResultsWithAllStats.csv")
sample <- sample[1:100,]
sampx <- cbind.data.frame(sample$maxEntPrediction,
sample$maxEntProbability,
sample$svmPrediction,
sample$svmProbability,
sample$glmnetPrediction,
sample$glmnetProbability,
sample$consensusCount,
sample$probabilityLabel,
sample$consensusLabel,
#allLabeled$python_sentiment,
sample$ensemble_pos,
sample$ensemble_neg,
sample$lexicon_pos,
sample$lexicon_neu,
sample$ensemble_neg,
sample$lexicon_compound)
colnames(sampx) <- c("maxEntPrediction", "maxEntProbability", "svmPrediction", "glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
pred <- predict(model, sampx)
sampy <- sample$X.correct..human.label
firsty <- gsub("1", "NotNeutral", sampy)
firsty <- gsub("3", "NotNeutral", firsty)
firsty <- gsub("2", "Neutral", firsty)
sampy <- as.factor(firsty)
print( table(pred, sampy) )
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral()
allLabeled <- read.csv("labeledWithPredictionsQuoteEdit.csv", sep=",")
if (tryJustNeutralOrNot)
{
# Basically turn this into an 'is neutral' flag
first <-  gsub("Negative", "NotNeutral", allLabeled$sentiment)
second <- gsub("Positive", "NotNeutral", first)
#y <- as.numeric(as.factor(third))
y <- as.factor(second)
}
else {
y <- as.factor(allLabeled$sentiment)
}
print(paste("# rows read in = ", dim(allLabeled)[1]))
x <- cbind.data.frame(allLabeled$maxEntPrediction,
allLabeled$maxEntProbability,
allLabeled$svmPrediction,
allLabeled$svmProbability,
allLabeled$glmnetPrediction,
allLabeled$glmnetProbability,
allLabeled$consensusCount,
allLabeled$probabilityLabel,
allLabeled$consensusLabel,
#allLabeled$python_sentiment,
allLabeled$ensemble_pos,
allLabeled$ensemble_neg,
allLabeled$lexicon_pos,
allLabeled$lexicon_neu,
allLabeled$ensemble_neg,
allLabeled$lexicon_compound)
colnames(x) <- c("maxEntPrediction", "maxEntProbability", "svmPrediction", "glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
# sentiment,predictedLabel,actualLabel,maxEntPrediction,maxEntProbability,svmPrediction,svmProbability,glmnetPrediction,glmnetProbability,consensusCount,probabilityLabel,consensusLabel,python_sentiment,ensemble_pos,ensemble_neg,lexicon pos,lexicon_neu,ensemble_neg,lexicon_compound
# TODO: we have a bad value around 8625/8626
#x <- x[1:8600,]
#y <- y[1:8600]
#second <- second[1:8600]
model <- svm(x, y)
pred <- predict(model, x)
print(paste("Predictions made!"))
predAndY <- cbind.data.frame(pred, y, second)
print( table(pred, y) )
sample <- read.csv("Sample_CombinedResultsWithAllStats.csv")
sample <- sample[1:100,]
sampx <- cbind.data.frame(sample$maxEntPrediction,
sample$maxEntProbability,
sample$svmPrediction,
sample$svmProbability,
sample$glmnetPrediction,
sample$glmnetProbability,
sample$consensusCount,
sample$probabilityLabel,
sample$consensusLabel,
#allLabeled$python_sentiment,
sample$ensemble_pos,
sample$ensemble_neg,
sample$lexicon_pos,
sample$lexicon_neu,
sample$ensemble_neg,
sample$lexicon_compound)
colnames(sampx) <- c("maxEntPrediction", "maxEntProbability", "svmPrediction", "glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
pred <- predict(model, sampx)
sampy <- sample$X.correct..human.label
sampy <- as.factor(sampy)
y[1:10]
print( table(pred, sampy) )
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral()
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral()
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral()
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral()
sampy <- sample$X.correct..human.label
sampy[1]
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral()
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral()
sampy <- sample$X.correct..human.label
if (tryJustNeutralOrNot == TRUE)
{
firsty <- gsub("1", "NotNeutral", as.character(sampy))
firsty <- gsub("3", "NotNeutral", as.character(firsty))
firsty <- gsub("2", "Neutral", as.character(firsty))
newsampy <- as.factor(firsty)
}
else {
firsty <- gsub("1", "Negative", as.character(sampy))
firsty <- gsub("3", "Positive", as.character(firsty))
firsty <- gsub("2", "Neutral", as.character(firsty))
newsampy <- as.factor(sampy)
}
sampy <- sample$X.correct..human.label
firsty <- gsub("1", "Negative", as.character(sampy))
firsty <- gsub("3", "Positive", as.character(firsty))
firsty <- gsub("2", "Neutral", as.character(firsty))
newsampy <- as.factor(sampy)
newsampy
sampy <- sample$X.correct..human.label
sampy[1]
as.character(sampy)[1,]
as.character(sampy)[1]
firsty <- gsub("1", "Negative", as.character(sampy))
firsty[1]
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral()
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral()
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral(TRUE)
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
colnames(sampx) <- c("maxEntPrediction", "maxEntProbability", "svmPrediction", "glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "consensusLabel",
"ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
colnames(x)
colnames(sampx)
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
colnames(x) <- c("maxEntPrediction", "maxEntProbability",
"svmPrediction", "svmnetProbability",
"glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "consensusLabel",
"ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
colnames(x)
tryLabellingJustNeutral(TRUE)
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryJustNeutralOrNot=TRUE
allLabeled <- read.csv("labeledWithPredictionsQuoteEdit.csv", sep=",")
if (tryJustNeutralOrNot == TRUE)
{
# Basically turn this into an 'is neutral' flag
first <-  gsub("Negative", "NotNeutral", allLabeled$sentiment)
second <- gsub("Positive", "NotNeutral", first)
#y <- as.numeric(as.factor(third))
y <- as.factor(second)
}
else {
y <- as.factor(allLabeled$sentiment)
}
print(paste("# rows read in = ", dim(allLabeled)[1]))
x <- cbind.data.frame(allLabeled$maxEntPrediction,
allLabeled$maxEntProbability,
allLabeled$svmPrediction,
allLabeled$svmProbability,
allLabeled$glmnetPrediction,
allLabeled$glmnetProbability,
allLabeled$consensusCount,
allLabeled$probabilityLabel,
allLabeled$consensusLabel,
#allLabeled$python_sentiment,
allLabeled$ensemble_pos,
allLabeled$ensemble_neg,
allLabeled$lexicon_pos,
allLabeled$lexicon_neu,
allLabeled$ensemble_neg,
allLabeled$lexicon_compound)
colnames(x) <- c("maxEntPrediction", "maxEntProbability",
"svmPrediction", "svmnetProbability",
"glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "consensusLabel",
"ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
# sentiment,predictedLabel,actualLabel,maxEntPrediction,maxEntProbability,svmPrediction,svmProbability,glmnetPrediction,glmnetProbability,consensusCount,probabilityLabel,consensusLabel,python_sentiment,ensemble_pos,ensemble_neg,lexicon pos,lexicon_neu,ensemble_neg,lexicon_compound
# TODO: we have a bad value around 8625/8626
#x <- x[1:8600,]
#y <- y[1:8600]
#second <- second[1:8600]
svmModel <- svm(x, y)
pred <- predict(svmModel, x)
predAndY <- cbind.data.frame(pred, y, second)
print(paste("Predictions on labeled august and march:"))
print( table(pred, y) )
sample <- read.csv("Sample_CombinedResultsWithAllStats.csv")
sample <- sample[1:100,]
sampx <- cbind.data.frame(sample$maxEntPrediction,
sample$maxEntProbability,
sample$svmPrediction,
sample$svmProbability,
sample$glmnetPrediction,
sample$glmnetProbability,
sample$consensusCount,
sample$probabilityLabel,
sample$consensusLabel,
#allLabeled$python_sentiment,
sample$ensemble_pos,
sample$ensemble_neg,
sample$lexicon_pos,
sample$lexicon_neu,
sample$ensemble_neg,
sample$lexicon_compound)
colnames(x) <- c("maxEntPrediction", "maxEntProbability",
"svmPrediction", "svmnetProbability",
"glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "consensusLabel",
"ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
print(paste("Predictions on labeled sample from March:"))
pred <- predict(svmModel, sampx)
sampy <- sample$X.correct..human.label
if (tryJustNeutralOrNot == TRUE)
{
firsty <- gsub("1", "NotNeutral", as.character(sampy))
firsty <- gsub("3", "NotNeutral", as.character(firsty))
firsty <- gsub("2", "Neutral", as.character(firsty))
newsampy <- as.factor(firsty)
}
else {
firsty <- gsub("1", "Negative", as.character(sampy))
firsty <- gsub("3", "Positive", as.character(firsty))
firsty <- gsub("2", "Neutral", as.character(firsty))
newsampy <- as.factor(firsty)
}
print( table(pred, newsampy) )
tryJustNeutralOrNot
allLabeled <- read.csv("labeledWithPredictionsQuoteEdit.csv", sep=",")
y <- as.factor(allLabeled$sentiment)
if (tryJustNeutralOrNot == TRUE)
{
# Basically turn this into an 'is neutral' flag
first <-  gsub("Negative", "NotNeutral", allLabeled$sentiment)
second <- gsub("Positive", "NotNeutral", first)
#y <- as.numeric(as.factor(third))
y <- as.factor(second)
}
print(paste("# rows read in = ", dim(allLabeled)[1]))
x <- cbind.data.frame(allLabeled$maxEntPrediction,
allLabeled$maxEntProbability,
allLabeled$svmPrediction,
allLabeled$svmProbability,
allLabeled$glmnetPrediction,
allLabeled$glmnetProbability,
allLabeled$consensusCount,
allLabeled$probabilityLabel,
allLabeled$consensusLabel,
#allLabeled$python_sentiment,
allLabeled$ensemble_pos,
allLabeled$ensemble_neg,
allLabeled$lexicon_pos,
allLabeled$lexicon_neu,
allLabeled$ensemble_neg,
allLabeled$lexicon_compound)
colnames(x) <- c("maxEntPrediction", "maxEntProbability",
"svmPrediction", "svmnetProbability",
"glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "consensusLabel",
"ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
# sentiment,predictedLabel,actualLabel,maxEntPrediction,maxEntProbability,svmPrediction,svmProbability,glmnetPrediction,glmnetProbability,consensusCount,probabilityLabel,consensusLabel,python_sentiment,ensemble_pos,ensemble_neg,lexicon pos,lexicon_neu,ensemble_neg,lexicon_compound
# TODO: we have a bad value around 8625/8626
#x <- x[1:8600,]
#y <- y[1:8600]
#second <- second[1:8600]
svmModel <- svm(x, y)
pred <- predict(svmModel, x)
predAndY <- cbind.data.frame(pred, y, second)
print(paste("Predictions on labeled august and march:"))
print( table(pred, y) )
sample <- read.csv("Sample_CombinedResultsWithAllStats.csv")
sample <- sample[1:100,]
sampx <- cbind.data.frame(sample$maxEntPrediction,
sample$maxEntProbability,
sample$svmPrediction,
sample$svmProbability,
sample$glmnetPrediction,
sample$glmnetProbability,
sample$consensusCount,
sample$probabilityLabel,
sample$consensusLabel,
#allLabeled$python_sentiment,
sample$ensemble_pos,
sample$ensemble_neg,
sample$lexicon_pos,
sample$lexicon_neu,
sample$ensemble_neg,
sample$lexicon_compound)
colnames(x) <- c("maxEntPrediction", "maxEntProbability",
"svmPrediction", "svmnetProbability",
"glmnetPrediction", "glmnetProbability",
"consensusCount", "probabilityLabel", "consensusLabel",
"ensemble_pos", "ensemble_neg", "lexicon_pos",
"lexicon_neu"  , "ensemble_neg", "lexicon_compound")
print(paste("Predictions on labeled sample from March:"))
pred <- predict(svmModel, sampx)
sampy <- sample$X.correct..human.label
if (tryJustNeutralOrNot == TRUE)
{
firsty <- gsub("1", "NotNeutral", as.character(sampy))
firsty <- gsub("3", "NotNeutral", as.character(firsty))
firsty <- gsub("2", "Neutral", as.character(firsty))
newsampy <- as.factor(firsty)
}
if (tryJustNeutralOrNot == FALSE)
{
firsty <- gsub("1", "Negative", as.character(sampy))
firsty <- gsub("3", "Positive", as.character(firsty))
firsty <- gsub("2", "Neutral", as.character(firsty))
newsampy <- as.factor(firsty)
}
print( table(pred, newsampy) )
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral(TRUE)
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral(TRUE)
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
tryLabellingJustNeutral(TRUE)
doInstall <- TRUE
toInstall <- c("maps", "ggplot2")
if(doInstall){install.packages(toInstall, repos = "http://cran.us.r-project.org")}
lapply(toInstall, library, character.only = TRUE)
library(ggplot2)
library(maps)
install.packages(toInstall, repos = "http://cran.us.r-project.org")
install.packages(toInstall, repos = "http://cran.us.r-project.org")
install.packages(toInstall, repos = "http://cran.us.r-project.org")
install.packages(toInstall, repos = "http://cran.us.r-project.org")
source('~/GitHub/data-mining-debate-analysis/R/tweetClassification.R', encoding = 'UTF-8')
