read.csv(
"c:\\users\\doylerav\\Documents\\github\\data-mining-debate-analysis\\R\\augSentimentforRmini.csv",
header = TRUE)
corpus <- Corpus(VectorSource(sentiment$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus
nGramLength
nGramLength = 1
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = nGramLength, max = nGramLength))
docTerms <- DocumentTermMatrix(corpus,
control=list(weighting=weightTfIdf,
tokenize = xgramTokenizer))
docTerms
docTerms[1,]
dim(docTerms)
dim(sentiment)
VectorSource(sentiment$text)
inspect(docTerms)
inspect(docTerms[1,])
sentiment <-
read.csv(
"c:\\users\\doylerav\\onedrive\\cs6220\\project\\SentimentforR.csv",
header = TRUE
)
numRows <- as.matrix(dim(sentiment))[1,1]
endTrain <- as.integer(.8 * numRows)
trainRows <- 1:endTrain
testRows <-    (endTrain+1):numRows
docTerms <- create_matrix(
sentiment$text,
language = "english",
removeStopwords = FALSE,  # run2 = false
minWordLength = 3,
ngramLength = nGramLength,  # run 1/2/3 = unigrams
weighting = tm::weightTfIdf,  # run1/2 = weightTf
removeNumbers = TRUE,
stemWords = FALSE,
toLower = TRUE,
removePunctuation = TRUE
)
inspect(docTerms[1,])
corpus <- Corpus(VectorSource(curFold$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
inspect(corpus[1,])
inspect(corpus)
length(corpus)
length(corpus[1])
inspect(corpus[1])
inspect(corpus[2])
inspect(corpus[3])
inspect(corpus[1])
corpus <- Corpus(VectorSource(sentiment$text))
inspect(corpus)
inspect(corpus[4])
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = nGramLength, max = nGramLength))
docTerms <- DocumentTermMatrix(corpus,
control=list(weighting=weightTfIdf,
tokenize = xgramTokenizer))
inspect(corpus)
inspect(docTerms)
inspect(corpus[4,])
inspect(corpus[4])
as.character(as.character(corpus[[4]]))
as.character(as.character(corpus[[1]]))
as.character(as.character(corpus[[5]]))
as.character(as.character(corpus[[6]]))
as.character(as.character(corpus[[7]]))
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
as.character(as.character(corpus[[8]]))
as.character(as.character(corpus[[9]]))
as.character(as.character(corpus[[10]]]))
as.character(as.character(corpus[[10:100]]))
as.character(as.character(corpus[[11]]))
as.character(as.character(corpus[[12]]))
as.character(as.character(corpus[[13]]))
as.character(as.character(corpus[[4]]))
as.character(as.character(corpus[[70]]))
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
corpus <- Corpus(VectorSource(sentiment$text))
corpus <- tm_map(corpus, removePunctuation)
toSpace <- content_transformer(function(x,pattern)
gsub(pattern," ", x))
# August data has this one odd charater for truncation
corpus <- tm_map(corpus,toSpace,"…")
corpus <- tm_map(corpus,toSpace, "\n")
corpus <- tm_map(corpus,toSpace,"\t")
corpus <- tm_map(corpus,toSpace,"\r")
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
as.character(as.character(corpus[[11]]))
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
corpus <- Corpus(VectorSource(sentiment$text))
toSpace <- content_transformer(function(x,pattern)
gsub(pattern," ", x))
# August data has this one odd charater for truncation
# Also, want to force certain word separators to a space
# so we extract words on either side
corpus <- tm_map(corpus,toSpace,"…")
corpus <- tm_map(corpus,toSpace, "\n")
corpus <- tm_map(corpus,toSpace,"\t")
corpus <- tm_map(corpus,toSpace,"\r")
# now collapse whitespace and remove
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
as.character(as.character(corpus[[11]]))
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
corpus <- Corpus(VectorSource(sentiment$text))
toSpace <- content_transformer(function(x,pattern)
gsub(pattern," ", x))
# August data has this one odd charater for truncation
# Also, want to force certain word separators to a space
# so we extract words on either side
corpus <- tm_map(corpus,toSpace,"…")
corpus <- tm_map(corpus,toSpace, "\n")
corpus <- tm_map(corpus,toSpace,"\t")
corpus <- tm_map(corpus,toSpace,"\r")
corpus <- tm_map(corpus,toSpace,".#")
corpus <- tm_map(corpus,toSpace,".@")
# now collapse whitespace and remove
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
as.character(as.character(corpus[[11]]))
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugTweetsRun
tryAugTweetsRun()
corpus <- Corpus(VectorSource(sentiment$text))
toSpace <- content_transformer(function(x,pattern)
gsub(pattern," ", x))
# August data has this one odd charater for truncation
# Also, want to force certain word separators to a space
# so we extract words on either side
#corpus <- tm_map(corpus,toSpace,"…")
corpus <- tm_map(corpus,toSpace, "\n")
corpus <- tm_map(corpus,toSpace,"\t")
corpus <- tm_map(corpus,toSpace,"\r")
#corpus <- tm_map(corpus,toSpace,".#")
#corpus <- tm_map(corpus,toSpace,".@")
# now collapse whitespace and remove
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
nGramLength
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = nGramLength, max = nGramLength))
docTerms <- DocumentTermMatrix(corpus,
control=list(weighting=weightTfIdf,
tokenize = xgramTokenizer))
container = create_container(
docTerms,
as.numeric(as.factor(curFold$sentiment)),
trainSize = trainRows,
testSize = testRows,
virgin = FALSE
)
container = create_container(
docTerms,
as.numeric(as.factor(sentiment$sentiment)),
trainSize = trainRows,
testSize = testRows,
virgin = FALSE
)
algos = c("MAXENT", "GLMNET", "SVM")
cat("Running ", algos, "...")
models = train_models(container, algorithms = algos)
results = classify_models(container, models)
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugTweetsRun()
as.character(as.character(corpus[[11]]))
as.character(as.character(corpus[[12]]))
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
corpus <- Corpus(VectorSource(sentiment$text))
toSpace <- content_transformer(function(x,pattern)
gsub(pattern," ", x))
# August data has this one odd charater for truncation
# Also, want to force certain word separators to a space
# so we extract words on either side
#corpus <- tm_map(corpus,toSpace,"…")
corpus <- tm_map(corpus,toSpace, "\n")
corpus <- tm_map(corpus,toSpace,"\t")
corpus <- tm_map(corpus,toSpace,"\r")
corpus <- tm_map(corpus, toSpace, "RT")
#corpus <- tm_map(corpus,toSpace,".#")
#corpus <- tm_map(corpus,toSpace,".@")
# now collapse whitespace and remove punc
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
as.character(as.character(corpus[[12]]))
as.character(as.character(corpus[[44]]))
as.character(as.character(corpus[[4]]))
tryAugTweetsRun()
corpus <- tm_map(corpus, toLower)
corpus <- tm_map(corpus, tolower)
as.character(as.character(corpus[[4]]))
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
corpus <- Corpus(VectorSource(sentiment$text))
toSpace <- content_transformer(function(x,pattern)
gsub(pattern," ", x))
# August data has this one odd charater for truncation
# Also, want to force certain word separators to a space
# so we extract words on either side
#corpus <- tm_map(corpus,toSpace,"…")
corpus <- tm_map(corpus,toSpace, "\n")
corpus <- tm_map(corpus,toSpace,"\t")
corpus <- tm_map(corpus,toSpace,"\r")
corpus <- tm_map(corpus, toSpace, "RT ")
#corpus <- tm_map(corpus,toSpace,".#")
#corpus <- tm_map(corpus,toSpace,".@")
# now collapse whitespace and remove punc
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
# You can examine what this did to any tweet like so:
#as.character(as.character(corpus[[4]]))
as.character(as.character(corpus[[4]]))
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = nGramLength, max = nGramLength))
docTerms <- DocumentTermMatrix(corpus,
control=list(weighting=weightTfIdf,
tokenize = xgramTokenizer))
tryAugTweetsRun()
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugTweetsRun()
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = nGramLength, max = nGramLength))
docTerms <- DocumentTermMatrix(corpus,
control=list(tolower=tolower,weighting=weightTfIdf,
tokenize = xgramTokenizer))
corpus <- Corpus(VectorSource(sentiment$text))
toSpace <- content_transformer(function(x,pattern)
gsub(pattern," ", x))
# August data has this one odd charater for truncation
# Also, want to force certain word separators to a space
# so we extract words on either side
#corpus <- tm_map(corpus,toSpace,"…")
corpus <- tm_map(corpus,toSpace, "\n")
corpus <- tm_map(corpus,toSpace,"\t")
corpus <- tm_map(corpus,toSpace,"\r")
corpus <- tm_map(corpus, toSpace, "RT ")
#corpus <- tm_map(corpus,toSpace,".#")
#corpus <- tm_map(corpus,toSpace,".@")
# now collapse whitespace and remove punc
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
#corpus <- tm_map(corpus, tolower)
# You can examine what this did to any tweet like so:
#as.character(as.character(corpus[[4]]))
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = nGramLength, max = nGramLength))
docTerms <- DocumentTermMatrix(corpus,
control=list(tolower=tolower,weighting=weightTfIdf,
tokenize = xgramTokenizer))
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugTweetsRun()
docTermMatrix
docTermMatrix$Terms
inspect(docTermMatrix)
inspect(docTermMatrix[[1]])
summary(docTermMatrix)
summary(docTerms)
docTerms
inspect(docTerms)
docTerms$ncol
docTerms$nrow
docTerms[,1]
docTerms[,2]
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral(doJustOneFold=FALSE)
tm <-removeSparseTerms(docTerms, sparse=.97) #removing really rare words so that the DTM is not too big
# Take a look at the values of columns 20-30 for all three docs
inspect(tm[1:3, 20:30])  # rows 1-3, terms 20-30
# Take a look a terms that show up at least twice
findFreqTerms(tm,2)
findFreqTerms(tm, 3)
findFreqTerms(tm, 4)
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
findFreqTerms(tm,100)
findFreqTerms(tm,1000)
findFreqTerms(tm,900)
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
corpusTest <- Corpus(VectorSource(sentiment$text))
corpus = tm_map(corpusTest, tolower)
toSpace <- content_transformer(function(x,pattern)
gsub(pattern," ", x))
removeIt <- content_transformer(function(x, pattern)) gsub(pattern, "", x))
removeIt <- content_transformer(function(x, pattern) gsub(pattern, "", x))
corpus <- tm_map(corpus, removeIt,  "http\\w+")
corpusTest <- gsub("http\\w+", "", sentiment$text)
corpusTest <- Corpus(VectorSource(corpusTest))
corpus <- tm_map(corpusTest,toSpace, "\n")
summary(docTerms)
head(docTerms)
docTerms
corpus <- tm_map(corpus,toSpace,"\t")
corpus <- tm_map(corpus,toSpace,"\r")
corpus <- tm_map(corpus, removeIt, "RT @")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = nGramLength, max = nGramLength))
docTerms <- DocumentTermMatrix(corpus,
control=list(tolower=tolower,
weighting=weightTfIdf,
tokenize = xgramTokenizer))
docTerms
as.character(as.character(corpus[[4]]))
as.character(as.character(corpus[[5]]))
docTerms[5]
inspect(docTerms[5])
inspect(docTerms)
head(inspect(docTerms))
as.character(as.character(corpus[[2]]))
as.character(as.character(corpus[[3]]))
as.character(as.character(corpusTest[[3]]))
corpusTest <- Corpus(VectorSource(sentiment$text))
as.character(as.character(corpusTest[[2]]))
corpusTest <- gsub("http:\\w+", "", sentiment$text)
as.character(as.character(corpusTest[[2]]))
corpusTest <- tm_map(corpusTest, removePunctuation)
corpusTest <- sentiment$text
corpusTest <- tm_map(corpusTest, removePunctuation)
libary(tm)
library(tm)
corpusTest <- tm_map(corpusTest, removePunctuation)
corpus <- Corpus(VectorSource(sentiment$text))
toSpace <- content_transformer(function(x,pattern)
gsub(pattern," ", x))
removeIt <- content_transformer(function(x, pattern)
gsub(pattern, "", x))
# Force certain word separators to a space
# so we extract words on either side
#other ideas: kill links, kill truncated items at end of tweet?
#corpus <- tm_map(corpus,toSpace,"…")
corpus <- tm_map(corpus,toSpace, "\n")
corpus <- tm_map(corpus,toSpace,"\t")
corpus <- tm_map(corpus,toSpace,"\r")
corpus <- tm_map(corpus, removeIt, "RT @")
#corpus <- tm_map(corpus,toSpace,".#")
#corpus <- tm_map(corpus,toSpace,".@")
# now collapse whitespace and remove punc
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
as.character(as.character(corpusTest[[2]]))
corpus <- tm_map(corpus, removeIt, "http\\w+")
as.character(as.character(corpusTest[[2]]))
as.character(as.character(corpus[[2]]))
corpus <- tm_map(corpus,toSpace,"…")
as.character(as.character(corpus[[2]]))
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = nGramLength, max = nGramLength))
docTerms <- DocumentTermMatrix(corpus,
control=list(tolower=tolower,
weighting=weightTfIdf,
tokenize = xgramTokenizer))
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
docTerms
testString <- as.character(as.character(corpusTest[[2]]))
testString
testString <- gsub("\\s*\\w*\\…$", "", testString)
testString
testString <- as.character(as.character(corpusTest[[2]]))
corpus <- Corpus(VectorSource(sentiment$text))
corpus <- tm_map(corpus,toSpace, "\n")
corpus <- tm_map(corpus,toSpace,"\t")
corpus <- tm_map(corpus,toSpace,"\r")
corpus <- tm_map(corpus, removeIt, "RT @")
#corpus <- tm_map(corpus,toSpace,".#")
#corpus <- tm_map(corpus,toSpace,".@")
# now collapse whitespace and remove punc
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeIt, "http\\w+")
as.character(as.character(corpus[[4]]))
as.character(as.character(corpus[[2]]))
as.character(as.character(corpus[[4]]))
as.character(as.character(corpus[[5]]))
as.character(as.character(corpus[[6]]))
as.character(as.character(corpus[[7]]))
as.character(as.character(corpus[[8]]))
as.character(as.character(corpus[[9]]))
as.character(as.character(corpus[[10]]))
as.character(as.character(corpus[[11]]))
as.character(as.character(corpus[[12]]))
as.character(as.character(corpus[[13]]))
as.character(as.character(corpus[[14]]))
as.character(as.character(corpus[[15:20]]))
as.character(as.character(corpus[[16]]))
corpus <- tm_map(corpus, removeIt, "…")
as.character(as.character(corpus[[7]]))
as.character(as.character(corpus[[27]]))
as.character(as.character(corpus[[28]]))
corpus <- Corpus(VectorSource(sentiment$text))
corpus <- tm_map(corpus,toSpace, "\n")
corpus <- tm_map(corpus,toSpace,"\t")
corpus <- tm_map(corpus,toSpace,"\r")
corpus <- tm_map(corpus, removeIt, "RT @")
#corpus <- tm_map(corpus,toSpace,".#")
#corpus <- tm_map(corpus,toSpace,".@")
# now collapse whitespace and remove punc
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
# Remove links
corpus <- tm_map(corpus, removeIt, "http\\w+")
corpus <- tm_map(corpus,removeIt,"\\s*\\w*\\…$")
as.character(as.character(corpus[[27]]))
as.character(as.character(corpus[[28]]))
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
as.character(as.character(corpus[[4]]))
docTerms <- DocumentTermMatrix(corpus,
control=list(tolower=tolower,
weighting=weightTfIdf,
tokenize = xgramTokenizer))
docTerms
tryAugNoNeutral()
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral()
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral()
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugTweetsRun()
docTerms
docTerms <- removeSparseTerms(docTerms, sparse=.98)
docTerms
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugTweetsRun()
docTerms <- DocumentTermMatrix(corpus,
control=list(tolower=tolower,
weighting=weightTfIdf,
tokenize = xgramTokenizer))
docTerms <- removeSparseTerms(docTerms, sparse=0.2)
docTerms
docTerms <- DocumentTermMatrix(corpus,
control=list(tolower=tolower,
weighting=weightTfIdf,
tokenize = xgramTokenizer))
docTerms <- removeSparseTerms(docTerms, sparse=0.99999)
docTerms
docTerms <- DocumentTermMatrix(corpus,
control=list(tolower=tolower,
weighting=weightTfIdf,
tokenize = xgramTokenizer))
docTerms
docTerms <- removeSparseTerms(docTerms, sparse=0.999)
docTerms
docTerms <- DocumentTermMatrix(corpus,
control=list(tolower=tolower,
weighting=weightTfIdf,
tokenize = xgramTokenizer))
docTerms <- removeSparseTerms(docTerms, sparse=0.9999)
docTerms
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral()
1/13871
1/13871
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral()
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral()
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral()
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral()
stripWhitespace()
stripWhitespace
stripWhitespace("what is going on ")
stripWhitespace("what is going on    ")
stripWhitespace("what is going on    \n")
stripWhitespace("what is going on\n")
stripWhitespace("what is going on\nson")
removeIt("RT @whatever")
removeIt("RT @whatever", "RT @")
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral()
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral()
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral()
print(docTerms)
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral(verbose=TRUE, doJustOneFold=FALSE)
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral(verbose=TRUE)
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral(verbose=TRUE)
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral(verbose=TRUE)
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral(verbose=TRUE)
tryAugNoNeutral(verbose=TRUE)
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
source('C:/Users/doylerav/OneDrive/r/tweetClassification.R')
tryAugNoNeutral(verbose=TRUE, doJustOneFold=FALSE)
setwd("~/GitHub/data-mining-debate-analysis/R")
trainAndEvaluate()
source('tweetClassification.R', encoding="UTF-8")
setwd("~/GitHub/data-mining-debate-analysis/R")
trainAndEvaluate()
